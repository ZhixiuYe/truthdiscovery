\documentclass[../../main.tex]{subfiles}
\begin{document}

In this section, the framework and results of the previous section are
evaluated. In particular, the framework is evaluated with respect to the
criteria outlined in section \ref{sec:theory_approach}.

\subsection*{Ability to model existing approaches}

The main definitions are that of a truth discovery network and a truth
discovery operator. For the framework to be useful as tool for analysing truth
discovery, these definitions should be compatible with the existing ideas and
approaches to truth discovery, in the sense that it should be possible to
define existing algorithms within the framework.

For the input network definition, it is easily verified that the definition
given is capable of modelling the input required for many algorithms proposed
in the literature. Indeed, since there is little disagreement on the form on
input across algorithms, there are several possible choices for the exact form
of the input, and the one we make is sufficient.

For the operator definition, we must consider whether the output of an
operator, namely a pair of total preorders on the set of sources and facts, is
sufficient to model the existing approaches. As mentioned previously, output
usually consists of numeric trust scores for each source, and either numeric
belief scores for facts or a single `true' fact for each object.

Whilst neither of these options involve rankings of sources and facts directly,
they both \emph{induce} such rankings, allowing both forms of output to be
reduced to a common form. Indeed, it was already noted that the numeric
scores induce rankings by simply sorting sources and facts by their score in
ascending order.

When given instead an identified `true' fact for each object, a ranking is
induced by having the identified true facts rank equally to each other and
strictly above non-true facts. For some algorithms, the identified true fact
may not have been claimed by any source; this is not a problem in
the framework since we permit a network to contain facts with no associated
sources. One may simply take the set of facts for an object to be \emph{all}
permitted values for the object \footnote{We make the assumption that the
domain of all possible values is well-defined as a set.}.

The definition of an operator can therefore model many existing algorithms.
However it neglects an important characteristic of many algorithms in practise,
which is that they operate \emph{iteratively}, running until the results
converge in some sense. For this reason, we defined an \emph{iterative
operator}.

This allowed a real-world algorithm, Sums, to be defined and analysed in the
framework. Due to time constraints, no other algorithms were realised. However
it is clear that many other algorithms can be defined in a similar way. This
is immediate for algorithms similar to Sums, such as \emph{Average$\cdot$Log},
\emph{Investment} and \emph{PooledInvestment} \cite{pasternack}, since they
only differ in their formulae for trust and belief score updates; their
fundamental method of operation is the same.

\subsection*{Simplicity}

Simplicity is naturally a subjective aim, since what appears simple to the
author may not appear so to others. Nonetheless, we argue that the framework
achieves its goal of expressing ideas as simply as possible.

For example, one of the key definitions is that of truth discovery network.
Adopting a graph-theoretic approach, the definition (including the constraints
on the graph) is easy to understand for those familiar with the basics of graph
theory, and even lends itself to pictorial representations of truth discovery
networks.

The next main definition is that of a truth discovery operator. This is defined
simply as a mapping from a space of inputs, denoted $\N$, to a space of
outputs, denoted $\orderings(\S) \times \orderings(\F)$. The definition of an
iterative operator extends the non-iterative one in a natural way, by defining
it simply as a sequence of non-iterative operators.

Whilst the notation for the rankings for a particular operator and particular
network may appear crowded at first, it expresses all the components of the
ranking without having to introduce additional notation prior to its use each
time. It is inspired by the notation introduced by Altman \& Tennenholtz
\cite{altman_foundations} for ranking systems.

We also believe that the axioms are expressed as simply as possible. Where the
formalities become tedious, plain-English explanations are provided to give
insight into the intuition backing them.

\subsection*{Flexibility}

Flexibility is also not something that can be objectively verified.
Nevertheless, we were able to express a variety of ideas in the framework
without excessive complexity, and the basic results shown have simple proofs.

\subsection*{Generality}

By and large, the framework is neutral with respect to any specific idea or
approach for truth discovery. A possible exception is perhaps the definition of
an iterative operator; this is defined as a sequence of \emph{numerical}
operators, whereas in principle an iterative algorithm need not compute
numerical scores. Indeed, algorithms such as $\mathsf{CRH}$ \cite{li_conflicts}
operate in an iterative manner, yet do not assign belief scores to facts.

However, the definition could easily be generalised to a sequence of
non-numerical operators, and a separate definition given for numerical
iterative operators. The definition as given was chosen to reduce the number of
definitions required and improve the clarity of the work, since the only
algorithm actually discussed \emph{does} in fact use numeric scores.

Another aim for the framework was to permit comparison between truth discovery
and related areas in the literature. The framework is general enough for this;
whilst clearly being a framework for truth discovery, one may easily see truth
discovery networks and operators from the perspective of social choice and
ranking systems. For example, it is easily seen that truth discovery networks
form a particular class of graphs, and a truth discovery operator is
essentially a ranking system defined on this class of graphs. The similarity is
also demonstrated empirically by the fact that many of the developed axioms are
directly inspired by axioms in these areas, but still have intuitive
interpretations in terms of truth discovery. However, the similarities do not
extend to areas less influenced by social choice, such as argumentation theory
and belief revision.

Having evaluated the definitions comprising the framework, we turn to the work
carried out inside it, namely the development of axioms and analysis of
operators with respect to these axioms.

\subsection*{Axioms and Results}

Several axioms covering a range of ideas were defined, each accompanied by a
description of the intuition backing them. It is hoped that the axioms
represent `desirable' properties for operators, although of course desirability
is a subjective property.

However, little work was done beside stating the axioms. An important aspect of
the axiomatic approach is to analyse the \emph{implications} of axioms, and to
consider interactions between them (e.g. impossibility and representation
results, or interesting properties entailed by a combination of axioms). In
section \ref{sec:theory_framework} only very simple results regarding the
axioms were proved, such as the independence of similar axioms and
incompatibility of source-symmetry and dictatorship. More work is required to
fully study the developed axioms.

In terms of analysis of operators with respect to the axioms, a set of sound
axioms for Sums was obtained. Whilst I expect that these axioms are not
\emph{complete}, this was not considered in section \ref{sec:theory_framework}.

A clear weakness of the analysis is that only one real-world algorithm is
considered. One of the aims for the framework was a unified model that can
represent many different algorithms -- defining only a single algorithm does
not demonstrate this particularly well.

As such, there is no comparison of the theoretical properties \emph{between
operators}. An interesting task would be to find axioms that distinguish
between operators, i.e. axioms that one operators satisfies but another does
not. This would provide insight into meaningful differences between operators,
which is hard to glean from the definitions in terms of an iterative procedure.
Knowledge of the differences in terms of simple desirable properties could be
helpful in deciding which algorithm to use in practise for real applications of
truth discovery.

It was noted above that a strength of the framework is the scope for comparison
between truth discovery and other areas. A weakness of the analysis is that no
such comparison was \emph{actually carried out}, besides the casual
observations linking truth discovery to social choice and ranking systems. To
make the links more concrete, one could consider whether social welfare
functions, ranking systems, annotation aggregators etc. can be formulated as
truth discovery operators, or vice versa. No attempt at this was made in
section \ref{sec:theory_framework}.

\end{document}
