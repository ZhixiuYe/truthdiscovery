\documentclass[../main.tex]{subfiles}
\begin{document}

The fundamental problem of truth discovery is to resolve conflicts in a set of
claims from different sources. A na\"ive approach is to apply a \emph{majority
vote}, where the claim made by the largest number of sources is accepted.
Unfortunately, this is prone to yield poor results when the sources are not all
equally trustworthy. For example, the study in \cite{vosoughi} found that false
information on Twitter is shared more quickly and more widely than true
information. Applying a majority vote in the Twittersphere would therefore, in
many cases, select the \emph{false} information as correct.

The problem with the majority vote is that all sources are treated identically:
a claim from one source carries as much weight as a claim from any other. This
is contrary to how we judge the veracity of statements in everyday life, where
claims from trusted colleagues have considerably more weight that claims from
unknown persons (and especially people known to be \emph{untrustworthy}). Trust
can therefore be a valuable tool in resolving conflicting information, since
one expects that trustworthy sources are more likely to make accurate claims
than untrustworthy sources are.

Truth discovery therefore has two components: determining \emph{trust} and
\emph{belief} in sources and claims, and resolving conflicts in data. These are
tightly linked, since the trust evaluation is based on the claims in the input
data, and the claims to accept are based on the trust evaluation.

Presently we outline related areas in the literature that deal with resolving
conflicts and trust analysis, before providing some background on truth
discovery itself. Then we review existing work relevant to the practical and
theoretical components of the project.

\subsection{Related areas}
\subsubsection{Resolving conflicts in data}

There are numerous areas in the existing literature that deal with resolving
conflicts in data. In data mining, \emph{data fusion} considers aggregating
data from different sources into a single representation. Various approaches
have been suggested (see \cite{bleiholder} for a review): for example, taking a
majority vote (as discussed above), taking the most recent value as correct, or
ignoring objects entirely when a conflict exists.

Belief revision \cite{gardenfors} is set out in a logical framework, and
considers how to update a knowledge base upon receiving new information that
could cause the knowledge base to be inconsistent.

Argumentation theory takes an abstract view and considers a set of `arguments'
which conflict with each other (known as `attacking'). The structure of the
arguments is abstracted away, and only the network of which arguments attack
each other is considered. The aim is then to find sets of arguments that are
acceptable and consistent.

\todo{AA overview citation}

In a more general sense, social choice also deals with conflicts in data. Here
voters express preferences for a number of `alternatives' (e.g. candidates for
an election), and a social ordering of the alternatives is sought that reflects
the will of the voters. Difficulties may arise when there is no consensus among
the voters -- e.g. one voter's favourite outcome could be another's least
favourite. Although the notion of fairness present here is not applicable
to truth discovery, the issue of conflicts in the preferences of voters is
relevant.

\subsubsection{Trust analysis}

Trust has been studied in many different domains for different applications
(see \cite{momani_challa} for a survey). In the social sciences and economics,
trust between humans has been considered for the effects on economic
transactions. E-commerce sites such as eBay use the concept of trust and
reputation of sellers to inform buyers.

In wireless sensor networks, nodes communicate to relay sensor data and to help
route packets through the wireless network. Nodes are required to report
accurate data and behave cooperatively: this may fail due to system failure
(e.g. hardware problems) or malicious interference from an adversary. Various
approaches have been suggested for analysing the trust of nodes in such
networks (e.g. see section 4.4 in \cite{momani_challa}) to mitigate effects of
misbehaviour. Trust analysis is applied in a similar way in P2P and ad-hoc
mobile networks.

It should be noted that some of these approaches to evaluating trust compute
`local' measures of trust from the perspective of a particular node. For
example, a sensor in a network may evaluate the trust of its neighbours based
on its interactions with them. The trust assigned to a given node may therefore
vary as it is evaluated from different perspectives. This is not the case with
truth discovery, where we seek an objective `global' notion of trust in sources
based only on the claims made.

Other works do not aim to \emph{compute} trust, but instead use trust
relationships between `agents' in a multi-agent system for some purpose. One
example is trust-based recommendation systems \cite{andersen}, where an agent
is given a recommendation for an item of interest based on the recommendations
of the agents it trusts. Other examples include personalised ranking systems
\cite{altman_personalised}, trust-based argumentation \cite{tang} and
trust-based belief revision \cite{booth}.

The trust considered here is again from the perspective of a given agent.
Nevertheless ideas from these areas still may be relevant to truth discovery.

From a theoretical point of view, Marsh \cite{marsh} provides a formal model
for trust in multi-agent systems.

\subsection{Truth discovery background}
\subsection{Existing work}
\subsubsection{Software implementations}
Software\ldots
\subsubsection{Theoretical work}
Theory\ldots

\end{document}
