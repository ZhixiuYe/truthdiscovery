\documentclass{article}
\date{February 2019}
\author{Joe Singleton}
\title{
    Notes on theory of truth discovery algorithms
}

% Packages:
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsthm}
% \usepackage[top=1cm, bottom=1cm, left=2cm, right=2cm]{geometry}

% Settings
\hypersetup{
	colorlinks,
	citecolor={blue!80!black},
	urlcolor={blue!80!black},
	linkcolor={blue!80!black}
}

% Maths environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{plain}
\newtheorem{axiom}{Axiom}

\begin{document}
\maketitle
% \begin{multicols}{2}

\section{Frameworks for truth-discovery used in existing literature}

\subsection{Li et. al. survey}
\label{sec:li}

A 2015 survey by Li et. al. {\cite{li_survey}} defines truth-discovery as
follows.

\begin{itemize}
\item We have a set of sources $\mathcal{S}$, and a set of objects $\mathcal{O}$
\item Each source $s$ claims a value $v_{o}^{s}$ for some objects $o$ (not
necessarily \emph{all} objects)
\item Truth-discovery: compute a \emph{source weight} $w_s$ for each source,
and compute `truth values' $v_{o}^{*}$ for each object $o$
\item \emph{Single truth assumption}: there is exactly one true value for each
object. This may not always hold (e.g. consider which actors star in a film)
\end{itemize}

\subsection{Gupta and Han survey}
\label{sec:gupta}

From \cite{gupta_han_survey}, slightly re-phrased by me (e.g. paper calls
sources `providers', denotes confidence by $s$\ldots)

\begin{itemize}
\item Set of sources $\mathcal{S}$, objects $\mathcal{O}$
\item Each object $o$ has as associated set of \emph{facts} $F_o$. Write
$\mathcal{F} = \bigcup_{o \in \mathcal{O}}F_o$
\item Sources \emph{provide} facts about objects. They may provide facts about
multiple objects, but only one fact per object
\item Could be formalised similar to \ref{sec:li} as follows: $f_o^s \in F_o$
is the fact provided by source $s$ for object $o$, defined for a subset of
$\mathcal{O}$
\item Truth-discovery: compute \emph{source trusts} $t_s$ for each source $s$,
and \emph{fact confidence} $c_f$ for each fact $f$
\end{itemize}

Note that general set up is more or less equivalent to in \ref{sec:li}: facts
replace values. However the definition of a truth-discovery algorithm is
different: here each fact is given a confidence score, whereas in \ref{sec:li}
we only find the most believable fact for each object.

If each fact is given a confidence score then it is simple to find most
believed fact for each object by taking the fact with highest confidence.

\subsection{Latent Dirichlet Truth Discovery}
\label{sec:ldt}

The method proposed in \cite{zhang_qi_tang} is similar to both above approaches:

\begin{itemize}
\item Set of sources $\mathcal{S}$, objects $\mathcal{O}$, facts $F_o$ for each
$o \in \mathcal{O}$
\item Each source $s$ makes claim $\texttt{Cl}_o(s) \in F_o \cup
\{\oslash\}$ for the true fact for object $o$, where $\oslash$ is special
symbol to indicate no claim is made
\item Each source has a \emph{trustworthy component} and \emph{untrustworthy
component}
\item Truth-discovery:
    \begin{itemize}
    \item Compute probability that each fact $f$ is the true fact for its
    associated object (i.e. confidence in each fact?)
    \item Compute trustworthy and untrustworthy amounts in each source
    (trustworthy amount can be considered source trustworthiness, comparable to
    other approaches)
    \end{itemize}
\end{itemize}

\subsection{Pasternack and Roth}
\label{sec:past}

In \cite{pasternack}:

\begin{itemize}
\item Set of sources $\mathcal{S}$, each with a set of claims $C_s$ ($s \in
\mathcal{S}$). Write $\mathcal{C} = \bigcup_{s \in \mathcal{S}}{C_s}$
\item Each claim $c$ has \emph{mutual exclusion} set $M_c \subseteq
\mathcal{C}$: only one claim in each $M_c$ is true. Note that $c \in M_c$
\item Truth-discovery: compute source trusts $t_s$ and claim beliefs $b_c$
\end{itemize}

This is essentially the same as \ref{sec:gupta}, if we let each mutual
exclusion set be called an object, rename claims to facts, and the facts
provided by source $s$ are simply those in $C_s$. Something like the following.
$$
\mathcal{O} = \{M_c : c \in \mathcal{C}\},
\quad
F_{M_c} = M_c
$$

The only slight problem is that under this mapping a fact (\ref{sec:gupta}
sense) may be related to multiple objects, e.g. if $c \in M_{c'}$ but $M_c \ne
M_{c'}$.

\subsection{Galland et. al algorithms}
\label{sec:galland}

Several algorithms are proposed in \cite{galland}, and each uses the following
scheme, which is initially quite different from those above (again, slightly
re-phrased by me in places):

\begin{itemize}
\item Set of sources $\mathcal{S}$, facts $\mathcal{F}$, and the \emph{real
world} $W: \mathcal{F} \rightarrow \{\texttt{True}, \texttt{False}\}$
\item Each source $s$ is a partial mapping $\mathcal{F} \rightarrow
\{\texttt{True}, \texttt{False}\}$
\item Each source $s$ has an \emph{error factor} $\epsilon(s) \in [0, 1]$ (?)
that represents the likeliness of $s$ making a mistake on the truth value of
any given fact
\item Set of \emph{queries} $\mathcal{Q}$. Each fact $f$ has a \emph{reference
query} $ref(f) \in \mathcal{Q}$ (imagine that $q\in\mathcal{Q}$ is a question,
and $f$ answers the question in some way). Any query has exactly one true fact.
Formally, for each query $q$, let $F_q = \{f \in \mathcal{F} : ref(f) = q\}$.
For each $q$ there is exactly one $f \in F_q$ with $W(f) = \texttt{True}$ \item
Truth-discovery: compute source error factors $\epsilon(s)$, and an estimate
for the real world $W$ given as a mapping $\mathcal{F} \rightarrow [0,
1]$ where 1 represents true and 0 represents false.
\end{itemize}

Some important differences here:
\begin{itemize}
\item Sources may claim a fact is \emph{false}, which is not possible in the
other approaches
\item Objects are not first-class citizens
\item Notion of queries. The set of queries (along with the reference query for
each fact) is not given as input to algorithms; it is instead used to produce a
modified set of views, where for each source $s$ and each fact $f$ such that
$s(f) = \texttt{True}$, we add a new claim $s(f')=\texttt{False}$ for each
$f' \ne f$ such that $f'$ and $f$ relate to the same query. This idea is
applicable whenever the \emph{single truth assumption} is assumed, but only for
algorithms that consider negative facts. See section 3 of \cite{galland} for
details.
\end{itemize}

I think we can view this model as a special case of the one in \ref{sec:li},
where objects (\ref{sec:li} sense) are facts, and $v_f^s$ (\ref{sec:li} sense)
is given by $s(f)$ (\ref{sec:galland} sense, when this is defined).

This is a special case since the only claimed values are \texttt{True} and
\texttt{False}, whereas in \ref{sec:li} the values can be anything.

\section{Extensions to basic truth-discovery}
\begin{itemize}

\item Implications between facts about the same object{\cite{yin_han_yu}}
\item Incorporating prior knowledge{\cite{pasternack}}
\item Considering difficulty of facts{\cite{galland}}
\item Heterogeneous data (i.e. possible values for an object are not all
categorical, and can be different types){\cite{li_conflicts}}
\item Correlations between objects{\cite{yang}}
\item Considering that for some questions there is no true answer (objects have
no true facts){\cite{zhi}}
\item Supervised truth-discovery (some ground truths are
known){\cite{yin_supervised}}
\item Truth changing over time, and sources copying from other
sources{\cite{dong}}
\item Truth-discovery in data streams, where source claims arrive continuously
over time{\cite{zhao}}

\end{itemize}

\section{Model for the development of axioms}

The input seems to be essentially equivalent in all approaches, with the
exception of \ref{sec:galland} which has the concept of negative claims.

Output differs: in \ref{sec:li} a single true fact is selected for each object,
and in the others all facts are given a belief score in $[0, 1]$. Both
approaches ultimately \emph{rank} the facts of each object.

The framework I choose should be general enough that all (or at least most)
algorithms in the literature can fit into it. The framework will then
facilitate comparison between the algorithms.

Initial ideas of a graph-theoretic formulation:

% Input network definition
\begin{definition}

A \emph{truth-discovery network} consists of non-empty sets $\mathcal{S}$
(sources), $\mathcal{F}$ (facts), $\mathcal{O}$ (objects), and a directed graph
$G=(V, E)$ where $V = \mathcal{S} \cup \mathcal{F} \cup \mathcal{O}$, and $E
\subseteq (\mathcal{S} \times \mathcal{F}) \cup (\mathcal{F} \times
\mathcal{O})$ satisfies the following properties:

\begin{enumerate}

\item Each $f \in \mathcal{F}$ has a unique successor node in $\mathcal{O}$,
denoted $O_f$ (i.e. each fact relates to a single object)

\item For $s \in \mathcal{S}$ and $o \in \mathcal{O}$, there is at most one $f
\in \mathcal{F}$ such that $(s, f) \in E$ and $O_f = o$ (i.e. sources can only
claim one fact per object)

\end{enumerate}

For $o \in \mathcal{O}$ we denote by $F_o$ the set $\{f \in \mathcal{F} : O_f =
o \}$.

\end{definition}

Some initial ideas for what a truth-discovery algorithm is.

% Algorithm definition
\begin{definition}
\label{def:ordinal}

A \emph{truth-discovery algorithm} $A$ assigns to each truth-discovery network
a pair $(\preceq_\mathcal{S}^A, \preceq_\mathcal{F}^A)$ of total (weak)
orderings on the sets $\mathcal{S}$ and $\mathcal{F}$ respectively.

Sub- and super-scripts can be omitted when the context is clear. $s_1 \preceq
s_2$ means $s_2$ is ranked as \emph{more trustworthy} than $s_1$; $f_1 \preceq
f_2$ means $f_2$ is ranked as \emph{more believable} than $f_1$.

\end{definition}

\begin{definition}
\label{def:ordinal_per_object}

A \emph{truth-discovery algorithm} $A$ assigns to each truth-discovery network
a total ordering $\preceq_\mathcal{S}^A$ on the set of sources $\mathcal{S}$
(the \emph{trust ordering}) and a total ordering $\preceq_o^A$ on the set of
facts $F_o$ for each object $o \in \mathcal{O}$.

\end{definition}


\begin{definition}
\label{def:numerical}

A \emph{truth-discovery algorithm} $A$ assigns to each truth-discovery network
a \emph{source trust} mapping $\tau: \mathcal{S} \rightarrow [0, 1]$ and
\emph{fact belief} mapping $\phi: \mathcal{F} \rightarrow [0, 1]$.

\end{definition}

\begin{definition}
\label{def:most_believed}

A \emph{truth-discovery algorithm} $A$ assigns to each truth-discovery network
a set of \emph{true facts} $\mathcal{F}^* \subseteq \mathcal{F}$ that contains
exactly one element of $F_o$ for each object $o$ (plus source trusts as per one
of definition \ref{def:ordinal} or \ref{def:numerical})

\end{definition}

\textbf{Thoughts:}
\begin{itemize}

\item Definition \ref{def:ordinal} considers only the relative order of
sources/facts, rather than assigning numerical values (as real-world algorithms
do in practise). This follows ideas in PageRank axiomatisation{\cite{altman}},
axioms for personalised ranking systems{\cite{altman_personalised}} and I think
social choice in general (e.g. Arrow, 1951, p17).

Also, numerical trust scores often do not have semantic meaning, so only the
relative scores are important anyway (definitely no semantics in Pasternack and
Roth paper \cite{pasternack}, although maybe there is in probabilistic ones
like \emph{TruthFinder}\cite{yin_han_yu} and LDT\cite{zhang_qi_tang})

\item Definition \ref{def:ordinal_per_object} is almost the same as
\ref{def:ordinal}, but does not allow facts for different objects to be
compared.

\item Every algorithm that I have seen in practise gives output for trusts as
in definition \ref{def:numerical}. The majority of them also give fact belief
in this way. This also captures how much more trustworthy sources are than each
other, which is lost when only considering relative trustworthiness (e.g. the
difference between $\tau(s_1) = 0.9, \tau(s_2) = 0.89$ and $\tau(s_1) = 0.9,
\tau(s_2) = 0.01$).

Also, dealing with numerical trust/belief would allow for comparing trust
scores when input changes, e.g. `make this change to the graph, and the trust
of this particular $s$ may not decrease'

\item Some algorithms only give most believed values as output (not belief
score for each fact) as per definition \ref{def:most_believed} (e.g.
\cite{li_conflicts}).

\item Some algorithms may give most-believed values for facts that are not
claimed by any sources, e.g. if values are continuous could perform a weighted
average of claimed facts (I believe this is done in \cite{li_conflicts} for
continuous data types).

None of the definitions above would support that.

\end{itemize}

\section{Ideas for axioms}

\subsection{Existing work}

\begin{itemize}

\item \emph{Trust-based recommendation systems: an axiomatic
approach}{\cite{andersen}}. In a trust-based recommendation system
\emph{agents} trust other agents, and a subset of the agents (the
\emph{voters}) have expressed an opinion about some item of interest: votes are
either $+$ or $-$. The goal is to assign recommendations $+$, $-$ or $0$ to the
\emph{non-voters} using the trust relationships between agents.

The network here is \emph{homogeneous} as all nodes in the graph are agents,
and agents trust each other. This is in contrast with truth-discovery where
edges are between entities of different types.

The recommendations are also \emph{personalised} which is not applicable to
truth-discovery, since we want to find the global truths.

Nevertheless the paper gives some axioms that seem like they could be
applicable.

\item \emph{An Axiomatic Approach to Personalised Ranking
Systems}{\cite{altman_personalised}}. Agents trust other agents, represented as
directed edges in a graph. A personalised ranking system gives a ranking of the
agents \emph{for each agent}. Each ranking depends on which agents are trusted.

Has interesting transitivity-type axioms that sound highly applicable to
truth-discovery: if a fact $f_1$ has sources that are `more trusted' than the
sources of $f_2$ (where $f_1, f_2$ relate to the same object), then $f_1$
should be more believed than $f_2$.

E.g. similar to `quasi-transitivity' in the paper: if there is a bijection $p$
between sources for $f_1$ and $f_2$ such that $s$ is more trusted than $p(s)$
for all sources $s$ of $f_1$, then $f_1$ is believed more than $f_2$.

The paper allows this for $p$ injective but not necessarily surjective. I don't
think would work for truth-discovery sine the addition of a very bad source
could (righly) bring a fact's belief down, even if its other sources were
better.

There can be a dual property where sources and facts are swapped.

\item \emph{Group Recommendations: Axioms, Impossibilities, and Random
Walks}{\cite{lev}}. Set up is similar to trust-based, but recommendations are
given to \emph{sets} of agents.

\end{itemize}

It seems like there is a lot of existing work on things like ranking systems,
aggregating preferences of a group (social choice) which have similarities with
truth-discovery.

\subsection{Ideas for truth-discovery}
\begin{itemize}

\item Symmetry/isomorphism: isomorphic input should give isomorphic output

\item Independence of irrelevant claims: if a subset of sources, objects, facts
is not connected to the rest of the graph, then changes outside this subset
should not affect the results on the subset.

\item Consensus-type axiom: if for an object $o$ all sources claim a fact $f$,
then $f$ ranks highest across facts for $o$ (only makes sense if we allow facts
that are not claimed by anyone).

\item Principle that trustworthy sources claim believable facts, and vice
versa: see `quasi-transitivity' above and in \cite{altman_personalised}.

Roughly, this would involve taking two facts, and comparing their sources in
some way. If one set of sources is `more trustworthy', the corresponding fact
should be more believable than the other. This could be for two facts about the
same object, or any two facts.

The other direction would be similar.

\item Making many false claims should not improve trust ranking (this is the
problem that \emph{Average${\cdot}$Log} tackles)

\item If a source claims a new fact better than all its current ones, its trust
ranking should not get worse. Also, if the new fact is worse, its trust ranking
should not go up.

Dual axiom for facts: if a new source is added that ranks higher for trust than
existing sources for a fact, that fact's belief ranking should not get worse.
Similar for opposite situation.

\textbf{Note}: if using numerical trust like in definition \ref{def:numerical},
it is easy to compare trust for a fixed source/fact after edges in the input
change.

Otherwise only the relative position of sources can be judged as getting better
or worse. Something like the following would check if $s$ gets worse than
\emph{some} other source after a change: $s$ ranks worse in $G'$ than in $G$ if
there is $s' \in \mathcal{S}$ such that $s' \prec_{\mathcal{S}} s$ and $s
\preceq_{\mathcal{S}}' s'$.

Or could assign each source a number according to which position they rank, and
use this number to compare positions after a change in input.

\end{itemize}

\bibliography{references}{}
\bibliographystyle{plain}

% \end{multicols}
\end{document}
